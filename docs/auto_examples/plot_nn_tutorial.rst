
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_nn_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_nn_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_nn_tutorial.py:


==================================
Neural Network Quickstart Tutorial
==================================

We go through two popular neural network examples

    * `BERT <plot_nn_tutorial.html#bert-example>`_ 
    * `KimCNN <plot_nn_tutorial.html#kimcnn-example>`_ 

in this tutorial. Before we start, please download and decompress the data ``rcv1`` via the following commands::

    mkdir -p data/rcv1
    wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel/rcv1_topics_train.txt.bz2 -O data/rcv1/train.txt.bz2
    wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel/rcv1_topics_test.txt.bz2 -O data/rcv1/test.txt.bz2
    bzip2 -d data/rcv1/*.bz2


BERT Example
============

This step-by-step example shows how to train and test a BERT model via LibMultiLabel.


Step 1. Import the libraries
----------------------------

Please add the following code to your python3 script.

.. GENERATED FROM PYTHON SOURCE LINES 30-35

.. code-block:: default


    from libmultilabel.nn.data_utils import *
    from libmultilabel.nn.nn_utils import *
    from transformers import AutoTokenizer


.. GENERATED FROM PYTHON SOURCE LINES 36-42

Step 2. Setup device
--------------------
If you need to reproduce the results, please use the function ``set_seed``. 
For example, you will get the same result as you always use the seed ``1337``.

For initial a hardware device, please use ``init_device`` to assign the hardware device that you want to use.

.. GENERATED FROM PYTHON SOURCE LINES 42-46

.. code-block:: default


    set_seed(1337)
    device = init_device()  # use gpu by default


.. GENERATED FROM PYTHON SOURCE LINES 47-61

Step 3. Load and tokenize data
------------------------------------------
We assume that the ``rcv1`` data is located at the directory ``./data/rcv1``, 
and there exist the files ``train.txt`` and ``test.txt``.
You can utilize the function ``load_datasets()`` to load the data sets. 
By default, LibMultiLabel tokenizes documents, but the BERT model uses its own tokenizer. 
Thus, we must set ``tokenize_text=False``.
Note that ``datasets`` contains three sets: ``datasets['train']``, ``datasets['val']`` and ``datasets['test']``, 
where ``datasets['train']`` and ``datasets['val']`` are randomly splitted from ``train.txt`` with the ratio ``8:2``.

For the labels of the data, we apply the function ``load_or_build_label()`` to generate the label set.

For BERT, we utilize the API ``AutoTokenizer``, which is supported by ``Hugging Face``, for the word preprocessing setting.
We set other variables for word preprocessing as ``None``.

.. GENERATED FROM PYTHON SOURCE LINES 61-67

.. code-block:: default


    datasets = load_datasets('data/rcv1/train.txt', 'data/rcv1/test.txt', tokenize_text=False)
    classes = load_or_build_label(datasets)
    word_dict, embed_vecs = None, None 
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')


.. GENERATED FROM PYTHON SOURCE LINES 68-72

Step 4. Initialize a model
--------------------------

We use the following code to initialize a model.

.. GENERATED FROM PYTHON SOURCE LINES 72-89

.. code-block:: default


    model_name='BERT'
    network_config = {
        'dropout': 0.1,
        'lm_weight': 'bert-base-uncased',
    }
    learning_rate = 0.00003
    model = init_model(
        model_name=model_name,
        network_config=network_config,
        classes=classes,
        word_dict=word_dict,
        embed_vecs=embed_vecs,
        learning_rate=learning_rate,
        monitor_metrics=['Micro-F1', 'Macro-F1', 'P@1', 'P@3', 'P@5']
    )


.. GENERATED FROM PYTHON SOURCE LINES 90-101

* ``model_name`` leads ``init_model`` function to find a network model.
* ``network_config`` contains the configurations of a network model.
* ``classes`` is the label set of the data.
* ``init_weight``, ``word_dict`` and ``embed_vecs`` are not used on a bert-base model, so we can ignore them.
* ``moniter_metrics`` includes metrics you would like to track.


Step 5. Initialize a trainer
----------------------------

We use the function ``init_trainer`` to initialize a trainer. 

.. GENERATED FROM PYTHON SOURCE LINES 101-104

.. code-block:: default


    trainer = init_trainer(checkpoint_dir='runs/NN-example', epochs=15, val_metric='P@5')


.. GENERATED FROM PYTHON SOURCE LINES 105-112

In this example, ``checkpoint_dir`` is the place we save the best and the last models during the training. Furthermore, we set the number of training loops by ``epochs=15``, and the validation metric by ``val_metric = 'P@5'``.

Step 6. Create data loaders
---------------------------

In most cases, we do not load a full set due to the hardware limitation.
Therefore, a data loader can load a batch of samples each time.

.. GENERATED FROM PYTHON SOURCE LINES 112-126

.. code-block:: default


    loaders = dict()
    for split in ['train', 'val', 'test']:
        loaders[split] = get_dataset_loader(
            data=datasets[split],
            word_dict=word_dict,
            classes=classes,
            device=device,
            max_seq_length=512,
            batch_size=8,
            shuffle=True if split == 'train' else False,
            tokenizer=tokenizer
        )


.. GENERATED FROM PYTHON SOURCE LINES 127-133

This example loads three loaders, and the batch size is set by ``batch_size=8``. Other variables can be checked in `here <../api/nn.html#libmultilabel.nn.data_utils.get_dataset_loader>`_.

Step 7. Train and test a model
------------------------------

The bert model training process can be started via 

.. GENERATED FROM PYTHON SOURCE LINES 133-136

.. code-block:: default


    trainer.fit(model, loaders['train'], loaders['val'])


.. GENERATED FROM PYTHON SOURCE LINES 137-138

After the training process is finished, we can then run the test process by

.. GENERATED FROM PYTHON SOURCE LINES 138-141

.. code-block:: default


    trainer.test(model, dataloaders=loaders['test'])


.. GENERATED FROM PYTHON SOURCE LINES 142-153

The results should be similar to::

 {
     'Macro-F1': 0.569891024909958, 
     'Micro-F1': 0.8142925500869751, 
     'P@1':      0.9552904367446899, 
     'P@3':      0.7907078266143799, 
     'P@5':      0.5505486726760864
 }

Please get the full example code `here <https://github.com/ASUS-AICS/LibMultiLabel/tree/master/docs/examples/bert_quickstart.py>`_.

.. GENERATED FROM PYTHON SOURCE LINES 155-169

KimCNN Example
==============

This example shows how to train and test a KimCNN model via LibMultiLabel. 
We only list the steps that are different from the BERT example.


Step 3. Load and tokenize data
------------------------------------------

To run KimCNN, LibMultiLabel tokenizes documents and uses an embedding vector for each word. 
Thus, ``tokenize_text = True`` is set.

We choose ``glove.6B.300d`` from torchtext as embedding vectors. 

.. GENERATED FROM PYTHON SOURCE LINES 169-175

.. code-block:: default


    datasets = load_datasets('data/rcv1/train.txt', 'data/rcv1/test.txt', tokenize_text=True)
    classes = load_or_build_label(datasets)
    word_dict, embed_vecs = load_or_build_text_dict(dataset=datasets['train'], embed_file='glove.6B.300d')
    tokenizer = None
 

.. GENERATED FROM PYTHON SOURCE LINES 176-180

Step 4. Initialize a model
--------------------------

We consider the following settings for the KimCNN model.

.. GENERATED FROM PYTHON SOURCE LINES 180-199

.. code-block:: default


    model_name = 'KimCNN'
    network_config = {
        'embed_dropout': 0.2,
        'encoder_dropout': 0.2,
        'filter_sizes': [2, 4, 8],
        'num_filter_per_size': 128
    }
    learning_rate = 0.0003
    model = init_model(
        model_name=model_name,
        network_config=network_config,
        classes=classes,
        word_dict=word_dict,
        embed_vecs=embed_vecs,
        learning_rate=learning_rate,
        monitor_metrics=['Micro-F1', 'Macro-F1', 'P@1', 'P@3', 'P@5']
    )


.. GENERATED FROM PYTHON SOURCE LINES 200-211

The test results should be similar to::

 {
     'Macro-F1': 0.48948464335831743,
     'Micro-F1': 0.7769773602485657,
     'P@1':      0.9471677541732788,
     'P@3':      0.7772253751754761,
     'P@5':      0.5449321269989014,
 }

Please get the full example code `here <https://github.com/ASUS-AICS/LibMultiLabel/tree/master/docs/examples/kimcnn_quickstart.py>`_.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.000 seconds)


.. _sphx_glr_download_auto_examples_plot_nn_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_nn_tutorial.py <plot_nn_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_nn_tutorial.ipynb <plot_nn_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
